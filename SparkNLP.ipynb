{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "SparkNLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gowthambalachandhiran/Spark-NLP-NER/blob/main/SparkNLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lQJKLi_xd9i"
      },
      "source": [
        "# NER with BERT in Spark NLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNQCdY2fxd9p",
        "outputId": "90938422-0caf-4d4d-ddcf-c130d952b9d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!python -V"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qx8EqTmxd99"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sun_epCexd9-",
        "outputId": "ad89d629-89a2-4643-9660-2708032f6e9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "\n",
        "# Install java\n",
        "! apt-get update -qq\n",
        "! apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "! java -version\n",
        "\n",
        "# Install pyspark\n",
        "! pip install --ignore-installed pyspark==2.4.4\n",
        "\n",
        "# Install Spark NLP\n",
        "! pip install --ignore-installed spark-nlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_275\"\n",
            "OpenJDK Runtime Environment (build 1.8.0_275-8u275-b01-0ubuntu1~18.04-b01)\n",
            "OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)\n",
            "Processing /root/.cache/pip/wheels/ab/09/4d/0d184230058e654eb1b04467dbc1292f00eaa186544604b471/pyspark-2.4.4-py2.py3-none-any.whl\n",
            "Collecting py4j==0.10.7\n",
            "  Using cached https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.4\n",
            "Collecting spark-nlp\n",
            "  Using cached https://files.pythonhosted.org/packages/84/84/3f15673db521fbc4e8e0ec3677a019ba1458b2cb70f0f7738c221511ef32/spark_nlp-2.6.3-py2.py3-none-any.whl\n",
            "Installing collected packages: spark-nlp\n",
            "Successfully installed spark-nlp-2.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZU_oXqoxd-J"
      },
      "source": [
        "## Import libraries and download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uS_cj3fxd-L"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "import sparknlp\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.base import *"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oswekG8uxd-S"
      },
      "source": [
        "spark = sparknlp.start()\n",
        "spark = sparknlp.start(gpu=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYgsFgNBxd-a",
        "outputId": "3ef0639a-ea9f-477a-9927-9689d7dff2c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spark NLP version:  2.6.3\n",
            "Apache Spark version:  2.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQrEOjcPxd-i"
      },
      "source": [
        "def start(gpu=False):\n",
        "    builder = SparkSession.builder \\\n",
        "        .appName(\"Spark NLP\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .config(\"spark.driver.memory\", \"8G\") \\\n",
        "        .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\\\n",
        "        .config(\"spark.kryoserializer.buffer.max\", \"1000M\")\n",
        "    if gpu:\n",
        "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp-gpu_2.11:2.5.1\")\n",
        "    else:\n",
        "        builder.config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.1\")\n",
        "\n",
        "    return builder.getOrCreate()\n",
        "\n",
        "  \n",
        "spark = start(gpu=False)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2q8M6bSPxd-o",
        "outputId": "8b7fcf9b-45db-4b7f-d6a1-3574754c6e30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "\n",
        "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.train',\n",
        "           'eng.train')\n",
        "\n",
        "urlretrieve('https://github.com/JohnSnowLabs/spark-nlp/raw/master/src/test/resources/conll2003/eng.testa',\n",
        "           'eng.testa')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('eng.testa', <http.client.HTTPMessage at 0x7f2fe47f5278>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJKaxth06f8a"
      },
      "source": [
        "import pandas as pd\n",
        "ncbi=pd.read_csv('/content/training_dataframe.csv')\n",
        "ncbi['pos']='NN'"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAk-YLGm67Hx",
        "outputId": "8405554a-bc0d-415e-b8b9-ee746f773221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ncbi.columns"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Sentence', 'Token', 'Tag', 'pos'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0clKJq6SuUq"
      },
      "source": [
        "This disease data set is open data set and annotaded by body of NCBI data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToeNTS3S6x0y",
        "outputId": "e317f8cc-b703-4ad0-b9e7-75d1a448021d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "conll_lines=\"-DOCSTART- -X- -X- -O-\\n\\n\"\n",
        "save=0\n",
        "file_loc = '/content/test.txt'\n",
        "for sent, token, pos, label in zip(ncbi['Sentence'],ncbi['Token'],ncbi['pos'],ncbi['Tag']): \n",
        "    \n",
        "# If the sentence ID has changed, that means we are starting a new sentence. We have to add an empty line.\n",
        "    \n",
        "    if save!=sent:\n",
        "        conll_lines+='\\n'\n",
        "    \n",
        "# Save the conll line\n",
        "    \n",
        "    conll_lines += \"{} {} {} {}\\n\".format(token,pos,pos,label)\n",
        "    \n",
        "    save=sent\n",
        "    \n",
        "\n",
        "# Now print all of the lines to a text file\n",
        "\n",
        "with open(file_loc,'w') as txtfile:\n",
        "        \n",
        "    for line in conll_lines:\n",
        "        txtfile.write(line)\n",
        "\n",
        "txtfile.close()\n",
        "\n",
        "with open(file_loc,'r') as f:\n",
        "    lines=f.readlines()[0:25]\n",
        "f.close()\n",
        "lines\n"
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['-DOCSTART- -X- -X- -O-\\n',\n",
              " '\\n',\n",
              " '\\n',\n",
              " 'Identification NN NN O\\n',\n",
              " 'of NN NN O\\n',\n",
              " 'APC2 NN NN O\\n',\n",
              " ', NN NN O\\n',\n",
              " 'a NN NN O\\n',\n",
              " 'homologue NN NN O\\n',\n",
              " 'of NN NN O\\n',\n",
              " 'the NN NN O\\n',\n",
              " 'adenomatous NN NN B-Disease\\n',\n",
              " 'polyposis NN NN I-Disease\\n',\n",
              " 'coli NN NN I-Disease\\n',\n",
              " 'tumour NN NN I-Disease\\n',\n",
              " 'suppressor NN NN O\\n',\n",
              " '. NN NN O\\n',\n",
              " '\\n',\n",
              " 'The NN NN O\\n',\n",
              " 'adenomatous NN NN B-Disease\\n',\n",
              " 'polyposis NN NN I-Disease\\n',\n",
              " 'coli NN NN I-Disease\\n',\n",
              " '( NN NN I-Disease\\n',\n",
              " 'APC NN NN I-Disease\\n',\n",
              " ') NN NN I-Disease\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qLGVGiqxd-v",
        "outputId": "c530fa3c-ae7e-447c-cfb0-61028e6dd970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(\"/content/test.txt\") as f:\n",
        "    c=f.read()\n",
        "\n",
        "print (c[:500])"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-DOCSTART- -X- -X- -O-\n",
            "\n",
            "\n",
            "Identification NN NN O\n",
            "of NN NN O\n",
            "APC2 NN NN O\n",
            ", NN NN O\n",
            "a NN NN O\n",
            "homologue NN NN O\n",
            "of NN NN O\n",
            "the NN NN O\n",
            "adenomatous NN NN B-Disease\n",
            "polyposis NN NN I-Disease\n",
            "coli NN NN I-Disease\n",
            "tumour NN NN I-Disease\n",
            "suppressor NN NN O\n",
            ". NN NN O\n",
            "\n",
            "The NN NN O\n",
            "adenomatous NN NN B-Disease\n",
            "polyposis NN NN I-Disease\n",
            "coli NN NN I-Disease\n",
            "( NN NN I-Disease\n",
            "APC NN NN I-Disease\n",
            ") NN NN I-Disease\n",
            "tumour NN NN I-Disease\n",
            "- NN NN O\n",
            "suppressor NN NN O\n",
            "protein NN NN O\n",
            "controls NN NN O\n",
            "the NN NN O\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvlXaJSnS3jB"
      },
      "source": [
        "Creating conll format of data set for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvLrCxoVxd-1"
      },
      "source": [
        "## Building NER pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upnNhIsGxd-3",
        "outputId": "03e36123-9af5-4a11-b968-2aa1c419ef3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "training_data = CoNLL().readDataset(spark, '/content/test.txt')\n",
        "training_data.show(20)"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Identification of...|[[document, 0, 89...|[[document, 0, 89...|[[token, 0, 13, I...|[[pos, 0, 13, NN,...|[[named_entity, 0...|\n",
            "|The adenomatous p...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|Complex formation...|[[document, 0, 63...|[[document, 0, 63...|[[token, 0, 6, Co...|[[pos, 0, 6, NN, ...|[[named_entity, 0...|\n",
            "|In colon carcinom...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|Here , we report ...|[[document, 0, 76...|[[document, 0, 76...|[[token, 0, 3, He...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|\n",
            "|Mammalian APC2 , ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 8, Ma...|[[pos, 0, 8, NN, ...|[[named_entity, 0...|\n",
            "|Like APC , APC2 r...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, Li...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|\n",
            "|Human APC2 maps t...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 4, Hu...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
            "|                 3 .|[[document, 0, 2,...|[[document, 0, 2,...|[[token, 0, 0, 3,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|\n",
            "|APC and APC2 may ...|[[document, 0, 79...|[[document, 0, 79...|[[token, 0, 2, AP...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|A common MSH2 mut...|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 0, A,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|\n",
            "|The frequency , o...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|The mutation ( A ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|Although this mut...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 7, Al...|[[pos, 0, 7, NN, ...|[[named_entity, 0...|\n",
            "|In contrast , the...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|To investigate th...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 1, To...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|Within the Englis...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 5, Wi...|[[pos, 0, 5, NN, ...|[[named_entity, 0...|\n",
            "|In contrast , a c...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|These findings su...|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Th...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
            "|We calculated age...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, We...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ7ABEZaxd-9",
        "outputId": "0a9d57cf-2c74-460d-ba94-4e9574c15eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_data.count()"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3641"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rg7fJuume7Th",
        "outputId": "17ca2166-3276-448f-a412-3c9349945ac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "training_data.select(F.explode(F.arrays_zip('token.result','label.result')).alias(\"cols\")) \\\n",
        ".select(F.expr(\"cols['0']\").alias(\"token\"),\n",
        "        F.expr(\"cols['1']\").alias(\"ground_truth\")).groupBy('ground_truth').count()\\\n",
        "        .orderBy('count', ascending=False).show(100,truncate=False)"
      ],
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------+-----+\n",
            "|ground_truth|count|\n",
            "+------------+-----+\n",
            "|O           |72040|\n",
            "|I-Disease   |3375 |\n",
            "|B-Disease   |2971 |\n",
            "|nan         |1    |\n",
            "+------------+-----+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Kmn88rGlBjc"
      },
      "source": [
        "Training and Evaluating NerDL¶\n",
        "NerDL is a deep learning named entity recognition model in the SparkNLP library which does not require training data to contain parts-of-speech. It is a Bidirectional LSTM-CNN. For a more detailed overview of training a model using NerDL, you can check out this post. We've already loaded the BC5CDR-Chem test and train datasets. Now I can show you how to add Glove embeddings and save the test data as a parquet file before NerDL model training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn9BsueInLpa"
      },
      "source": [
        "Neural Network architecture is Char CNNs - BiLSTM - CRF that achieves state-of-the-art in most datasets.\n",
        "Output type: Named_Entity\n",
        "Input types: Document, Token, Word_Embeddings\n",
        "Reference: NerDLApproach | NerDLModel\n",
        "\n",
        "\n",
        "https://nlp.johnsnowlabs.com/docs/en/annotators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJL6dafYtxJu",
        "outputId": "3f57fc01-3cab-4931-8369-f870d1e83c66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "glove_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")\n",
        "nerTagger = NerDLApproach()\\\n",
        "        .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "        .setLabelColumn(\"label\")\\\n",
        "        .setOutputCol(\"ner\")\\\n",
        "        .setMaxEpochs(4)\\\n",
        "        .setLr(0.002)\\\n",
        "        .setPo(0.005)\\\n",
        "        .setBatchSize(16)\\\n",
        "        .setRandomSeed(0)\\\n",
        "        .setVerbose(1)\\\n",
        "        .setValidationSplit(0.2)\\\n",
        "        .setEvaluationLogExtended(True) \\\n",
        "        .setEnableOutputLogs(True)\\\n",
        "        .setIncludeConfidence(True)\\\n",
        "        .setOutputLogsPath('ner_logs')\n",
        "ner_pipeline_glove = Pipeline(stages=[\n",
        "    glove_embeddings,\n",
        "    nerTagger\n",
        "])"
      ],
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVrAIxE9ylwG"
      },
      "source": [
        "ner_model_glove = ner_pipeline_glove.fit(training_data)\n",
        "ner_model_glove.stages[1].write().overwrite().save(\"/content/Tr_NER_DL\")"
      ],
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvPoxstfzkeU"
      },
      "source": [
        "test_data = glove_embeddings.transform(test)\n",
        "predictions = ner_model_glove.transform(test_data)\n",
        "from sklearn.metrics import classification_report\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "preds = predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        "        .select(F.col('cols.0').alias(\"token\"),\n",
        "        F.col('cols.1').alias(\"label\"),\n",
        "        F.col('cols.2').alias(\"ner\"))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySw0HRH4qcSo"
      },
      "source": [
        "#Convert the Spark dataframe to a Pandas dataframe.\n",
        "import pandas as pd\n",
        "preds_df_dl =preds.toPandas()"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uulwgsSS0DFH",
        "outputId": "0072ca5b-25c4-41a0-a93d-2e2535a6deee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print (classification_report(preds_df_dl['label'], preds_df_dl['ner']))"
      ],
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.86      0.90      0.88      2971\n",
            "   I-Disease       0.92      0.83      0.88      3375\n",
            "           O       0.99      0.99      0.99     72040\n",
            "         nan       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.98     78387\n",
            "   macro avg       0.69      0.68      0.69     78387\n",
            "weighted avg       0.98      0.98      0.98     78387\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBfBzHIF057H",
        "outputId": "ba176441-c7e0-4881-9c1b-dfe62c3e8a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "  .setInputCol(\"text\")\\\n",
        "  .setOutputCol(\"document\")\n",
        "sentence = SentenceDetector()\\\n",
        "  .setInputCols(['document'])\\\n",
        "  .setOutputCol('sentence')\n",
        "token = Tokenizer()\\\n",
        "  .setInputCols(['sentence'])\\\n",
        "  .setOutputCol('token')\n",
        "glove_embeddings = WordEmbeddingsModel() \\\n",
        "  .pretrained('glove_100d')\\\n",
        "  .setInputCols([\"sentence\",'token'])\\\n",
        "  .setOutputCol(\"embeddings\")\\\n",
        "  .setCaseSensitive(True)\n",
        "loaded_ner_model = NerDLModel.load(\"/content/Tr_NER_DL\")\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "  .setOutputCol(\"ner\")\n",
        "converter = NerConverter()\\\n",
        "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
        "  .setOutputCol(\"ner_span\")\n",
        "ner_prediction_pipeline = Pipeline(\n",
        "  stages = [document,\n",
        "            sentence,\n",
        "            token,\n",
        "            glove_embeddings,\n",
        "            loaded_ner_model,\n",
        "            converter])"
      ],
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TE6cs8w1HMP",
        "outputId": "f5be9ef5-3e63-45f0-9ae5-ccc3bd3bf125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "empty_data = spark.createDataFrame([['']]).toDF(\"text\")\n",
        "prediction_model = ner_prediction_pipeline.fit(empty_data)\n",
        "prediction_model.transform(empty_data)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[text: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, sentence: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, embeddings: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, ner: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, ner_span: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 297
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7OHqqbp1bz2"
      },
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "light_model = LightPipeline(prediction_model)\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_highlighter.py\n",
        "import ner_highlighter"
      ],
      "execution_count": 298,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FLAVTet1irr"
      },
      "source": [
        "text = \"\"\"Coronavirus disease (COVID-19) is an infectious disease caused adenomatous  by a newly discovered coronavirus.\n",
        "\n",
        "Most people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people, and those with underlying medical problems like cardiovascular disease, diabetes, chronic respiratory disease, and cancer are more likely to develop serious illness.\n",
        "\n",
        "The best way to prevent and slow down transmission is to be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face. \n",
        "\"\"\""
      ],
      "execution_count": 299,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsvznPG41gdJ",
        "outputId": "94e04f4a-3c20-46aa-b0dd-6415a328e43d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "result = light_model.fullAnnotate(text)[0]\n",
        "ner_highlighter.chunk_highlighter(result, entity_column=\"ner_span\")"
      ],
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Coronavirus disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " (COVID-19) is an \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    infectious disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " caused \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    adenomatous\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              "  by a newly discovered coronavirus.</br></br>Most people infected with the COVID-19 virus will experience mild to moderate \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    respiratory illness\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " and recover without requiring special treatment.  Older people, and those with underlying medical problems like \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    cardiovascular disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    diabetes\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    chronic respiratory disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              ", and \n",
              "<mark class=\"entity\" style=\"background: #70A3DA; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    cancer\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " are more likely to develop serious illness.\n",
              "\n",
              "The best way to prevent and slow down transmission is to be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face. \n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLOMwXbkkqyQ"
      },
      "source": [
        "Bidirectional Encoder Representations from Transformers is a Transformer-based machine learning technique for natural language processing pre-training developed by Google."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olxzy0NzKKRi",
        "outputId": "0e38958e-ad14-4b5b-d498-2f957e4854b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bert = BertEmbeddings.pretrained() \\\n",
        " .setInputCols([\"sentence\", \"token\"])\\\n",
        " .setOutputCol(\"bert\")\\\n",
        " .setCaseSensitive(False)"
      ],
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "small_bert_L2_768 download started this may take some time.\n",
            "Approximate size to download 139.6 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWL8nwHJKTc2",
        "outputId": "b7df7909-028f-44f9-e84c-3d1e6760487c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "training_data.show()"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Identification of...|[[document, 0, 89...|[[document, 0, 89...|[[token, 0, 13, I...|[[pos, 0, 13, NN,...|[[named_entity, 0...|\n",
            "|The adenomatous p...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|Complex formation...|[[document, 0, 63...|[[document, 0, 63...|[[token, 0, 6, Co...|[[pos, 0, 6, NN, ...|[[named_entity, 0...|\n",
            "|In colon carcinom...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|Here , we report ...|[[document, 0, 76...|[[document, 0, 76...|[[token, 0, 3, He...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|\n",
            "|Mammalian APC2 , ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 8, Ma...|[[pos, 0, 8, NN, ...|[[named_entity, 0...|\n",
            "|Like APC , APC2 r...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, Li...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|\n",
            "|Human APC2 maps t...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 4, Hu...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
            "|                 3 .|[[document, 0, 2,...|[[document, 0, 2,...|[[token, 0, 0, 3,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|\n",
            "|APC and APC2 may ...|[[document, 0, 79...|[[document, 0, 79...|[[token, 0, 2, AP...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|A common MSH2 mut...|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 0, A,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|\n",
            "|The frequency , o...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|The mutation ( A ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|\n",
            "|Although this mut...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 7, Al...|[[pos, 0, 7, NN, ...|[[named_entity, 0...|\n",
            "|In contrast , the...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|To investigate th...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 1, To...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|Within the Englis...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 5, Wi...|[[pos, 0, 5, NN, ...|[[named_entity, 0...|\n",
            "|In contrast , a c...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "|These findings su...|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Th...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|\n",
            "|We calculated age...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, We...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yppuZZCOKkhf",
        "outputId": "c4003c14-37e4-4969-e6f3-2ae5b8cdf035",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# WARNING: Setting benchmark to true is  slow and might crash your system and is not recommended on standardCollab notebooks-- High end hardware and/or GPU required\n",
        "## dataframe.cache() does not solve this. Results must be serialized to disk for maximum efficiency\n",
        "### You might need to restart your driver after this step finishes\n",
        "benchmark = False \n",
        "\n",
        "\n",
        "with_bert_path = \"./with_bert.parquet\"\n",
        "if benchmark == True :\n",
        "  if not Path(with_bert_path).is_dir(): \n",
        "    bert.transform(training_data).write.parquet(\"./with_bert.parquet\")\n",
        "    training_with_bert = spark.read.parquet(\"./with_bert.parquet\").cache()\n",
        "else : training_with_bert = bert.transform(training_data)\n",
        "\n",
        "\n",
        "print(training_with_bert.count())\n",
        "training_with_bert.select(\"token\", \"bert\").show()"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3641\n",
            "+--------------------+--------------------+\n",
            "|               token|                bert|\n",
            "+--------------------+--------------------+\n",
            "|[[token, 0, 13, I...|[[word_embeddings...|\n",
            "|[[token, 0, 2, Th...|[[word_embeddings...|\n",
            "|[[token, 0, 6, Co...|[[word_embeddings...|\n",
            "|[[token, 0, 1, In...|[[word_embeddings...|\n",
            "|[[token, 0, 3, He...|[[word_embeddings...|\n",
            "|[[token, 0, 8, Ma...|[[word_embeddings...|\n",
            "|[[token, 0, 3, Li...|[[word_embeddings...|\n",
            "|[[token, 0, 4, Hu...|[[word_embeddings...|\n",
            "|[[token, 0, 0, 3,...|[[word_embeddings...|\n",
            "|[[token, 0, 2, AP...|[[word_embeddings...|\n",
            "|[[token, 0, 0, A,...|[[word_embeddings...|\n",
            "|[[token, 0, 2, Th...|[[word_embeddings...|\n",
            "|[[token, 0, 2, Th...|[[word_embeddings...|\n",
            "|[[token, 0, 7, Al...|[[word_embeddings...|\n",
            "|[[token, 0, 1, In...|[[word_embeddings...|\n",
            "|[[token, 0, 1, To...|[[word_embeddings...|\n",
            "|[[token, 0, 5, Wi...|[[word_embeddings...|\n",
            "|[[token, 0, 1, In...|[[word_embeddings...|\n",
            "|[[token, 0, 4, Th...|[[word_embeddings...|\n",
            "|[[token, 0, 1, We...|[[word_embeddings...|\n",
            "+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "CPU times: user 12.1 ms, sys: 83 µs, total: 12.2 ms\n",
            "Wall time: 2.99 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izPO9HbsKmw1"
      },
      "source": [
        "\n",
        "nerTagger = NerDLApproach()\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"bert\"])\\\n",
        "  .setLabelColumn(\"label\")\\\n",
        "  .setOutputCol(\"ner\")\\\n",
        "  .setMaxEpochs(1)\\\n",
        "  .setRandomSeed(0)\\\n",
        "  .setVerbose(0)\n",
        "\n",
        "converter = NerConverter()\\\n",
        "  .setInputCols([\"document\", \"token\", \"ner\"])\\\n",
        "  .setOutputCol(\"ner_span\")\n",
        "\n",
        "pipeline = Pipeline(\n",
        "    stages = [\n",
        "    nerTagger,\n",
        "    converter\n",
        "  ])"
      ],
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlDd1HS7KqAI",
        "outputId": "8af6661d-d0dc-4bb6-dd29-c929bff68618",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%%time\n",
        "import time\n",
        "start = time.time()\n",
        "print(\"Start fitting\")\n",
        "#We have to limit the rows in Collab, otherwise we will encounter exceptions because of RAM limitations\n",
        "model = pipeline.fit(training_with_bert)  \n",
        "print(\"Fitting is ended\")\n",
        "print (time.time() - start)"
      ],
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start fitting\n",
            "Fitting is ended\n",
            "415.52539229393005\n",
            "CPU times: user 114 ms, sys: 17.8 ms, total: 132 ms\n",
            "Wall time: 6min 55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YGPbxmMOpoi",
        "outputId": "7e0802b6-b0ee-4d55-87ab-8ca0ad4d9e89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "preds = predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        "        .select(F.col('cols.0').alias(\"token\"),\n",
        "        F.col('cols.1').alias(\"label\"),\n",
        "        F.col('cols.2').alias(\"ner\"))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|                text|            document|            sentence|               token|                 pos|               label|          embeddings|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|Identification of...|[[document, 0, 89...|[[document, 0, 89...|[[token, 0, 13, I...|[[pos, 0, 13, NN,...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|The adenomatous p...|[[document, 0, 21...|[[document, 0, 21...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Complex formation...|[[document, 0, 63...|[[document, 0, 63...|[[token, 0, 6, Co...|[[pos, 0, 6, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|In colon carcinom...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Here , we report ...|[[document, 0, 76...|[[document, 0, 76...|[[token, 0, 3, He...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Mammalian APC2 , ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 8, Ma...|[[pos, 0, 8, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Like APC , APC2 r...|[[document, 0, 18...|[[document, 0, 18...|[[token, 0, 3, Li...|[[pos, 0, 3, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Human APC2 maps t...|[[document, 0, 36...|[[document, 0, 36...|[[token, 0, 4, Hu...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|                 3 .|[[document, 0, 2,...|[[document, 0, 2,...|[[token, 0, 0, 3,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|APC and APC2 may ...|[[document, 0, 79...|[[document, 0, 79...|[[token, 0, 2, AP...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|A common MSH2 mut...|[[document, 0, 15...|[[document, 0, 15...|[[token, 0, 0, A,...|[[pos, 0, 0, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|The frequency , o...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|The mutation ( A ...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 2, Th...|[[pos, 0, 2, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Although this mut...|[[document, 0, 22...|[[document, 0, 22...|[[token, 0, 7, Al...|[[pos, 0, 7, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|In contrast , the...|[[document, 0, 12...|[[document, 0, 12...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|To investigate th...|[[document, 0, 23...|[[document, 0, 23...|[[token, 0, 1, To...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|Within the Englis...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 5, Wi...|[[pos, 0, 5, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|In contrast , a c...|[[document, 0, 13...|[[document, 0, 13...|[[token, 0, 1, In...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|These findings su...|[[document, 0, 14...|[[document, 0, 14...|[[token, 0, 4, Th...|[[pos, 0, 4, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "|We calculated age...|[[document, 0, 19...|[[document, 0, 19...|[[token, 0, 1, We...|[[pos, 0, 1, NN, ...|[[named_entity, 0...|[[word_embeddings...|\n",
            "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTqfSCx4K208"
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "prediction_pipeline = Pipeline(\n",
        "    stages = [\n",
        "        document,\n",
        "        sentence,\n",
        "        token,\n",
        "        bert,\n",
        "        model\n",
        "    ]\n",
        ")"
      ],
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydQF9ffsMfJH",
        "outputId": "3c9f9c01-4ce9-40b8-f3df-600c2c6dd21b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prediction_data = spark.createDataFrame([[\"\"]]).toDF(\"text\")\n",
        "prediction_model_bert = prediction_pipeline.fit(empty_data)\n",
        "prediction_model.transform(empty_data)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[text: string, document: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, sentence: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, token: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, bert: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, ner: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>, ner_span: array<struct<annotatorType:string,begin:int,end:int,result:string,metadata:map<string,string>,embeddings:array<float>>>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DLLIpcMMmG-"
      },
      "source": [
        "from sparknlp.base import LightPipeline\n",
        "light_model = LightPipeline(prediction_model_bert)\n",
        "!wget -q https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/utils/ner_highlighter.py\n",
        "import ner_highlighter"
      ],
      "execution_count": 302,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll5kWvGLMolw",
        "outputId": "254e7afa-af9e-40ce-ca00-308c85659701",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "result = light_model.fullAnnotate(text)[0]\n",
        "ner_highlighter.chunk_highlighter(result, entity_column=\"ner_span\")\n"
      ],
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #BF6AB3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Coronavirus disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " (COVID-19) is an \n",
              "<mark class=\"entity\" style=\"background: #BF6AB3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    infectious disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              " caused \n",
              "<mark class=\"entity\" style=\"background: #BF6AB3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    adenomatous\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              "  by a newly discovered coronavirus.</br></br>Most people infected with the COVID-19 virus will experience mild to moderate respiratory illness and recover without requiring special treatment.  Older people, and those with underlying medical problems like \n",
              "<mark class=\"entity\" style=\"background: #BF6AB3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    cardiovascular disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              ", diabetes, chronic \n",
              "<mark class=\"entity\" style=\"background: #BF6AB3; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    respiratory disease\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DISEASE</span>\n",
              "</mark>\n",
              ", and cancer are more likely to develop serious illness.\n",
              "\n",
              "The best way to prevent and slow down transmission is to be well informed about the COVID-19 virus, the disease it causes and how it spreads. Protect yourself and others from infection by washing your hands or using an alcohol based rub frequently and not touching your face. \n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLydKggqSoCC",
        "outputId": "edc8a3c1-4af4-4475-d267-acda7af42327",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "word_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")\n",
        "\n",
        "nerTagger = NerCrfApproach()\\\n",
        "    .setInputCols([\"sentence\", \"token\", \"pos\",\"embeddings\"])\\\n",
        "    .setLabelColumn(\"label\")\\\n",
        "    .setOutputCol(\"ner\")\\\n",
        "    .setMaxEpochs(9)\\\n",
        "    \n",
        "ner_pipeline = Pipeline(stages=[\n",
        "          word_embeddings,\n",
        "          nerTagger\n",
        " ])"
      ],
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4oDZVnfTCKZ"
      },
      "source": [
        "ner_model = ner_pipeline.fit(training_data)"
      ],
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwGYyYz6TJ6I"
      },
      "source": [
        "from sparknlp.training import CoNLL\n",
        "\n",
        "file_loc='/content/test.txt'\n",
        "test = CoNLL().readDataset(spark, file_loc)\n",
        "\n",
        "test_data = word_embeddings.transform(test)"
      ],
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XlFjESBTROi"
      },
      "source": [
        "predictions = ner_model.transform(test_data)"
      ],
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5PwP5HrUGTA"
      },
      "source": [
        "\n",
        "You can see all of your input and output columns in the final \"predictions\" dataframe, but I'll focus on the 'ner' column which contains the prediction, and the 'label' column which contains the ground truth. You can use sklearn.metrics classification_report to check the accuracy of the predictions using these 2 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elUX-9OVUHPw"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pyspark.sql.functions as F\n",
        "\n",
        "preds = predictions.select(F.explode(F.arrays_zip('token.result','label.result','ner.result')).alias(\"cols\")) \\\n",
        "        .select(F.col('cols.0').alias(\"token\"),\n",
        "        F.col('cols.1').alias(\"label\"),\n",
        "        F.col('cols.2').alias(\"ner\"))"
      ],
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDrr8TySUJiQ"
      },
      "source": [
        "#Convert the Spark dataframe to a Pandas dataframe.\n",
        "import pandas as pd\n",
        "preds_df=preds.toPandas()"
      ],
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPIqVdvFUMmY",
        "outputId": "d750548f-436d-4e6d-b790-20c0ae5ea3ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(classification_report(preds_df['label'], preds_df['ner']))"
      ],
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   B-Disease       0.92      0.74      0.82      2971\n",
            "   I-Disease       0.95      0.74      0.83      3375\n",
            "           O       0.98      1.00      0.99     72040\n",
            "         nan       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.98     78387\n",
            "   macro avg       0.71      0.62      0.66     78387\n",
            "weighted avg       0.98      0.98      0.98     78387\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozS4odwVVXjR"
      },
      "source": [
        "ner_model.stages[1].write().overwrite().save(\"crf_model_14_11_2020\")"
      ],
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMOVkORHUcCh",
        "outputId": "9d5fd425-5ddd-4ed6-b45f-3c7d10bab8e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "document = DocumentAssembler()\\\n",
        "    .setInputCol(\"text\")\\\n",
        "    .setOutputCol(\"document\")\n",
        "\n",
        "sentence = SentenceDetector()\\\n",
        "    .setInputCols(['document'])\\\n",
        "    .setOutputCol('sentence')\n",
        "\n",
        "token = Tokenizer()\\\n",
        "    .setInputCols(['sentence'])\\\n",
        "    .setOutputCol('token')\n",
        "\n",
        "word_embeddings = WordEmbeddingsModel.pretrained('glove_100d')\\\n",
        "          .setInputCols([\"document\", \"token\"])\\\n",
        "          .setOutputCol(\"embeddings\")\n",
        "\n",
        "loaded_ner_model = NerCrfModel.load(\"crf_model_14_11_2020\")\\\n",
        "  .setInputCols([\"sentence\", \"token\", \"embeddings\"])\\\n",
        "  .setOutputCol(\"ner\")\n",
        "\n",
        "ner_prediction_pipeline = Pipeline(\n",
        "  stages = [document,\n",
        "            sentence,\n",
        "            token,\n",
        "            word_embeddings,\n",
        "            loaded_ner_model])\n",
        "\n"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "glove_100d download started this may take some time.\n",
            "Approximate size to download 145.3 MB\n",
            "[OK!]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YFZciRSWRw8",
        "outputId": "7fa4ba0f-c76b-4fc8-9765-e80d841e20b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "prediction_data = spark.createDataFrame([[\"\"\"Coronavirus disease (COVID-19) is an infectious disease caused by a newly discovered coronavirus.\"\"\"]]).toDF(\"text\")\n",
        "prediction_data.show()"
      ],
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|                text|\n",
            "+--------------------+\n",
            "|Coronavirus disea...|\n",
            "+--------------------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ZgeWpoZfyC"
      },
      "source": [
        "prediction_model = ner_prediction_pipeline.fit(prediction_data)"
      ],
      "execution_count": 276,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3_I-XHeZ7KK",
        "outputId": "f45eb107-36b8-423d-c1bc-7406e931a9f0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "%%time\n",
        "\n",
        "lp = LightPipeline(prediction_model)\n",
        "result = lp.annotate(\"Patient was suffering from heart disease\")\n",
        "for e in list(zip(result['token'], result['ner'])):\n",
        "    print(e)"
      ],
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('Patient', 'O')\n",
            "('was', 'O')\n",
            "('suffering', 'O')\n",
            "('from', 'O')\n",
            "('heart', 'B-Disease')\n",
            "('disease', 'I-Disease')\n",
            "CPU times: user 51.8 ms, sys: 15 ms, total: 66.8 ms\n",
            "Wall time: 120 ms\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}